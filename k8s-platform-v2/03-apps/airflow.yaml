apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-webserver
  template:
    metadata:
      labels:
        app: airflow-webserver
    spec:
      securityContext:
        fsGroup: 50000
      initContainers:
      - name: wipe-old-files
        image: busybox
        # We delete everything to start fresh
        command: ["sh", "-c", "rm -rf /opt/airflow/dags/*"]
        volumeMounts:
        - name: dags-data
          mountPath: /opt/airflow/dags
      containers:
      - name: airflow-webserver
        image: apache/airflow:2.7.1
        args: ["webserver"]
        env:
        - name: AIRFLOW__CORE__EXECUTOR
          value: "LocalExecutor"
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          value: "postgresql+psycopg2://airflow:airflow@postgres.default.svc.cluster.local/airflow"
        - name: AIRFLOW__CORE__FERNET_KEY
          value: "46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho="
        - name: AIRFLOW__CORE__LOAD_EXAMPLES
          value: "False"
        - name: AIRFLOW__CORE__DAGS_FOLDER
          value: "/opt/airflow/dags/repo"
        volumeMounts:
        - name: dags-data
          mountPath: /opt/airflow/dags

      - name: git-sync
        # We use alpine/git to perform a REAL clone, not a 'link' sync
        image: alpine/git
        securityContext:
          runAsUser: 50000
        command:
        - /bin/sh
        - -c
        - |
          # 1. Clean up and Clone
          cd /opt/airflow/dags
          rm -rf repo
          git clone https://github.com/sp-202/airflow-dags.git repo
          
          # 2. Infinite Loop to Pull changes
          while true; do
            echo "Pulling latest code..."
            cd /opt/airflow/dags/repo && git pull
            sleep 60
          done
        volumeMounts:
        - name: dags-data
          mountPath: /opt/airflow/dags

      volumes:
      - name: dags-data
        persistentVolumeClaim:
          claimName: airflow-dags-pvc
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-scheduler
  template:
    metadata:
      labels:
        app: airflow-scheduler
    spec:
      securityContext:
        fsGroup: 50000
      initContainers:
      - name: wipe-old-files
        image: busybox
        command: ["sh", "-c", "rm -rf /opt/airflow/dags/*"]
        volumeMounts:
        - name: dags-data
          mountPath: /opt/airflow/dags
      containers:
      - name: airflow-scheduler
        image: apache/airflow:2.7.1
        args: ["scheduler"]
        env:
        - name: AIRFLOW__CORE__EXECUTOR
          value: "LocalExecutor"
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          value: "postgresql+psycopg2://airflow:airflow@postgres.default.svc.cluster.local/airflow"
        - name: AIRFLOW__CORE__FERNET_KEY
          value: "46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho="
        - name: AIRFLOW__CORE__DAGS_FOLDER
          value: "/opt/airflow/dags/repo"
        volumeMounts:
        - name: dags-data
          mountPath: /opt/airflow/dags

      - name: git-sync
        image: alpine/git
        securityContext:
          runAsUser: 50000
        command:
        - /bin/sh
        - -c
        - |
          cd /opt/airflow/dags
          rm -rf repo
          git clone https://github.com/sp-202/airflow-dags.git repo
          while true; do
            cd /opt/airflow/dags/repo && git pull
            sleep 60
          done
        volumeMounts:
        - name: dags-data
          mountPath: /opt/airflow/dags

      volumes:
      - name: dags-data
        persistentVolumeClaim:
          claimName: airflow-dags-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: airflow-webserver
  namespace: default
spec:
  ports:
  - port: 8080
    targetPort: 8080
    name: web
  selector:
    app: airflow-webserver